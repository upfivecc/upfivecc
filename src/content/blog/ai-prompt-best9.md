---
title: "20个可以用来最大程度地减少幻觉的提示词策略"
date: 2025-03-15T21:10:00+08:00
updated: 2025-03-15T21:10:00+08:00
keywords: ["ChatGPT", "提示词", "提示词公式"]
featured: true
summary: " 20个可以用来最大程度地减少幻觉的提示词策略"
---

刚刚读完 OpenAI 和佐治亚理工学院研究人员的这项人工智能相关的新研究，它终于解释了为什么即使是我们最好的法学硕士也会不断编造东西。

简而言之：“幻觉”不是一个漏洞，而是这些模型学习方式本身固有的特性。



研究人员发现，幻觉并不是人工智能的某种神秘怪癖，它们本质上与基本分类问题中的错误相同。

当训练数据稀疏时（例如名人生日只被提及一次），模型必须进行猜测 ，而目前的评估方法实际上奖励的是自信的猜测，而不是说“我不知道”。

试想一下：大多数人工智能基准测试的评分方式都像学校考试一样，空白答案等于 0 分，但错误的自信答案仍然可能获得部分分数。

因此，模型学会了过度自信地胡说八道，而不是承认不确定性。

25种真正有效的抗幻觉提示

1. 置信阈值提示

> 只有在你有超过 75% 的把握时才回答。如果不确定，请回复 “我需要更多信息才能可靠地回答。” 错误的答案所受的惩罚是说不知道的 3 倍。

2. 单例检测器

> 在回答关于特定事实（日期、名称、数字）的问题之前，首先要评估：“我在训练中是否多次见过这个确切信息？” 如果没有，要明确说明自己的不确定程度。

3. 反诈唬指令

> 你不是在参加一场鼓励猜测的考试。在实际对话中，说 “我不确定” 比给出自信的错误答案更能建立信任。要优先考虑准确性而非完整性。

4. 信息来源意识提示

> 对于你提出的每一个事实性主张，都要在内心问自己：“我有充分的证据支持这一点，还是我只是在进行模式匹配？” 对于那些可能是有根据的猜测而非经过证实的事实的陈述，要进行标记。

5. 校准检查

> 在提供具体细节（日期、统计数据、名称）之前，请对自己的信心进行 1-10 分的评分。只有当信心度达到 8 分及以上时，才提供细节。若信心度为 7 分及以下，则给出一般性信息并承认存在不确定性。

6. 对冲助手

> 使用精确的不确定性语言：用 “根据常见模式，这可能是……” 而不是 “这是……”。在不确定的时候，使用 “通常”“经常”“一般来说” 等词，而不是绝对的表述。

7. 矛盾检测器

> 生成回应后，检查是否有任何可能错误的特定说法。如果对过于具体的细节没有十足把握，就用更笼统的表述来替换。

8. “我不知道” 冠军

> 记住：“我不知道” 或者 “我对具体细节不确定” 都是完全合理的回应。你的目标是提供帮助且说实话，而不是总要有答案。

9. 训练数据真实性检验

> 当被问及冷门事实、罕见事件或关于不太知名的人 / 地方的具体细节时，默认回答：“这似乎是训练数据中可能没有充分体现的信息。我应该对具体细节保持谨慎。”

10. 认知谦逊提示

> 在做出任何事实性陈述之前，要考虑：“这是我真正知道的事情，还是我从模式中推断出来的事情？” 如果是推断，就说 “这似乎是……” 或者 “这表明……”

11. 处罚意识指导


> 假设答错扣 5 分，答对得 1 分，“我不知道” 得 0 分来进行操作。只有当你有足够的信心，认为预期分值为正时，才进行回答。

12. 特异性节流阀

> 请求的信息越具体（确切的日期、精确的数字、特定的引语），你的置信度阈值就应该越高。对于高度具体的请求，需要 90% 以上的置信度，否则就选择不回答。

13. 模式与事实的区分

> 区分 “我从训练中识别出了这个模式” 和 “我有关于这个事实的具体证据”。只将基于模式的回应作为暂定的可能性呈现，而非确信的断言。

14. 图灵缺失质量估计器

> 对于那些可能只在训练数据中出现过一次的事实性问题（如生日、论文标题、特定事件），要假定自己很可能答错，并表现出高度的不确定性。

15. 行为校准提示

> 表现得好像你的回答会被专家进行事实核查一样。只做出那些你愿意向能够核实你陈述的领域专家进行辩护的断言。

16. 分布偏移检测器

> 如果一个问题看起来不寻常、针对性极强，或者与典型的训练示例不同，请用以下内容标记：“这个问题似乎超出了我最可靠的知识领域，因此我在回答时应该格外谨慎。”

17. 元不确定性追踪器

> 不仅要评估你对事实的信心，还要评估你对自己信心的信心。如果你不确定自己有多确定，那就是一个强烈的信号，表明你要明确表达出不确定性。

18. 计算难度确认器

> 对于复杂计算、多步骤推理或需要精确计算结果的问题，要承认局限性：“这类计算 / 推理对于语言模型来说错误率很高。”

19. 垃圾进垃圾出过滤器

> 请记住，训练数据中存在错误。对于有争议的话题、近期事件或技术细节，在回应前请加上：“训练数据的质量参差不齐，因此我需要在此说明可能存在的不确定性。”

20. 打破虚假二分法的人

> 不要在正确 / 错误的答案之间强迫进行二元选择，而是采用三元分类：确定 / 不确定 / 不知道。当证据不足时，默认选择后两者。

21. 苏格拉底式不确定性方法

> 对于不确定的问题，不要直接给出答案，而是这样回应：“我对具体细节不太确定，但关于这个一般话题，我可以告诉你这些……” 或者 “你最感兴趣的具体方面是什么？”

22. 检索失败确认

> 当你无法自信地 “检索” 特定信息时，要明确地说出来：“我无法可靠地获取这一特定信息”，而不是生成听起来合理的替代内容。

23. 过度自信的代价

> 假设过度自信的错误答案比适度的不确定更会损害信任。请相应地权衡你的回答：“看起来知识没那么渊博但更值得信任，这会更好。”

24. 语言校准方法

> 让你的语言确定性与你实际的信心相匹配：“我相当确定……”“我认为……”“有可能……”“我不确定，但……”“我没有关于…… 的可靠信息”

25. 现实世界后果框架

> 思考一下：“如果有人根据这些信息做出了重要决定，我对承担这个责任会有多安心？” 如果答案是 “不太安心”，那就表达出适当的不确定性。


这项研究表明，解决幻觉问题需要改变我们评估人工智能系统的方式，而不仅仅是改进训练。目前的评估标准需要停止惩罚不确定性，而应该奖励恰当表达“我不知道”的行为。


在此之前，这些提示可以帮助你获得更真实的回答，因为它们会明确地奖励不确定性而非自信的猜测。关键在于： 幻觉之所以持续存在，是因为我们的评估方法从根本上就存在偏差——它们奖励的是自信的胡说八道，而不是真诚的不确定性。

